{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyelasticsearch.client import ElasticSearch\n",
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_excel('data/overall-tf-idf.xlsx') # Your training dataset here\n",
    "df_test = pd.read_excel('data/overall-test-bsn.xlsx') # Your test set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dict = df_train.transpose().to_dict()\n",
    "test_dict = df_test.transpose().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_train_document = []\n",
    "for key in train_dict:\n",
    "    document = {}\n",
    "    document['id'] = key\n",
    "    document['label'] = train_dict[key]['Label']\n",
    "    document['tweet'] = train_dict[key]['Tweets']\n",
    "    list_of_train_document.append(document)\n",
    "    \n",
    "list_of_test_document = []\n",
    "for key in test_dict:\n",
    "    document = {}\n",
    "    document['id'] = key\n",
    "    document['label'] = test_dict[key]['Label']\n",
    "    document['tweet'] = test_dict[key]['Tweets']\n",
    "    list_of_test_document.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Index your document in ES\n",
    "def index_documents_in_ES(index, documents):\n",
    "    es = ElasticSearch()\n",
    "    for document in documents:\n",
    "        res = es.index(index=index, doc_type='tweet', id=document['id'], doc=document)\n",
    "        if res['created'] == False:\n",
    "            print res\n",
    "\n",
    "# Build your more like this query\n",
    "def build_mlt(nb, doc_id):\n",
    "    mlt = {}\n",
    "    mlt[\"from\"] = 0\n",
    "    mlt[\"size\"] = nb\n",
    "    mlt[\"query\"] = {}\n",
    "    mlt[\"query\"][\"more_like_this\"] = {}\n",
    "    mlt[\"query\"][\"more_like_this\"][\"fields\"] = [\"tweet\"]\n",
    "    mlt[\"query\"][\"more_like_this\"][\"like\"] = [{\"_index\" : \"test\",\"_type\" : \"tweet\",\"_id\" : doc_id}]\n",
    "    mlt[\"query\"][\"more_like_this\"][\"min_term_freq\"] = 1\n",
    "    mlt[\"query\"][\"more_like_this\"][\"max_query_terms\"] = 50\n",
    "    mlt[\"query\"][\"more_like_this\"][\"minimum_should_match\"] = \"25%\"\n",
    "    return mlt\n",
    "\n",
    "\n",
    "# Extract as a list the result from Elasticsearch\n",
    "def extract_from_json(json):\n",
    "    hits = json['hits']['hits']\n",
    "    documents = [hit['_source'] for hit in hits]\n",
    "    return documents\n",
    "\n",
    "#This function need to be changed in order to be more versatile\n",
    "def get_max_label(documents):\n",
    "    bullying = 0\n",
    "    sarcasm = 0\n",
    "    normal = 0\n",
    "    for document in documents:\n",
    "        if document['label'] == 'sarcasm':\n",
    "            sarcasm += 1\n",
    "        elif document['label'] == 'normal':\n",
    "            normal  += 1\n",
    "        else:\n",
    "            bullying += 1\n",
    "    max_label = max([sarcasm, bullying, normal])\n",
    "    if max_label == bullying:\n",
    "        return 'bully'\n",
    "    elif max_label == sarcasm:\n",
    "        return 'sarcasm'\n",
    "    else:\n",
    "        return 'normal'\n",
    "        \n",
    "# Send the request to elasticsearch and extract the result\n",
    "def get_similar(nb, doc_id):\n",
    "    mlt = build_mlt(nb, doc_id)\n",
    "    response = requests.post(\"http://localhost:9200/train/tweet/_search\", data=json.dumps(mlt))\n",
    "    similar_documents = extract_from_json(json.loads(response.text))\n",
    "    return similar_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_documents_in_ES('train', list_of_train_document)\n",
    "index_documents_in_ES('test', list_of_test_document)\n",
    "\n",
    "for document in list_of_test_document:\n",
    "    similar_documents = get_similar(25, document['id'])\n",
    "    label = get_max_label(similar_documents)\n",
    "    document['auto_label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_result = {}\n",
    "for document in list_of_test_document:\n",
    "    classifier_result[document['id']] = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(classifier_result)\n",
    "df_result.transpose()\n",
    "df_result.transpose().to_csv('result.csv', encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
